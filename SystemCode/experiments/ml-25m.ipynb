{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fcc318f-f205-498a-8699-06edd620be58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 23:13:15.781153: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 23:13:15.844947: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 23:13:16.143508: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-03-29 23:13:16.143538: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-03-29 23:13:16.143540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from deepmatch.models import ComiRec, NCF\n",
    "from deepmatch.utils import sampledsoftmaxloss, NegativeSampler\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "\n",
    "from deepctr.feature_column import DenseFeat\n",
    "\n",
    "pad_sequences = tf.keras.utils.pad_sequences\n",
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9281be-b3ed-44bc-b8e7-2261505b8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_input_for_user(user_df):\n",
    "    #user_df = user_df.sort_values(\"timestamp\")\n",
    "    # user_df = user_df[-SEQ_LEN:] \n",
    "    user_id = user_df[\"user_id\"].to_list()[0]\n",
    "    tmp = [user_df[:x+1] for x in range(len(user_df))]\n",
    "    input_data = []\n",
    "    for record in tmp:\n",
    "        sample = {\n",
    "            \"history_movie_id\":np.array(record[\"movie_id\"].tolist()[:-1]),\n",
    "            \"history_genre_id\": np.array(record[\"genres\"].tolist()[:-1]),\n",
    "            \"movie_id\":np.array(record[\"movie_id\"].tolist()[-1]),\n",
    "            # np.array(record[\"genres\"].tolist()[:-1]) # history genre list\n",
    "        }\n",
    "        input_data.append(sample)\n",
    "    # train, test = input_data[:-1], input_data[-1:]\n",
    "    result_df = pd.DataFrame(input_data)\n",
    "    result_df[\"label\"] = 1 # tmp for now\n",
    "    result_df[\"hist_len\"] = SEQ_LEN\n",
    "    result_df[[\"label\", \"hist_len\"]] = result_df[[\"label\", \"hist_len\"]].astype(\"int8\")\n",
    "    return result_df\n",
    "\n",
    "def get_test_input_for_user(user_df):\n",
    "    #user_df = user_df.sort_values(\"timestamp\")\n",
    "    # user_df = user_df[-SEQ_LEN:] \n",
    "    user_id = user_df[\"user_id\"].to_list()[0]\n",
    "    \n",
    "    input_data = []\n",
    "    record = user_df\n",
    "    sample = {\n",
    "        \"history_movie_id\":np.array(record[\"movie_id\"].tolist()[:-1]),\n",
    "        \"history_genre_id\": np.array(record[\"genres\"].tolist()[:-1]),\n",
    "        \"movie_id\":np.array(record[\"movie_id\"].tolist()[-1]),\n",
    "        # np.array(record[\"genres\"].tolist()[:-1]) # history genre list\n",
    "    }\n",
    "    input_data.append(sample)\n",
    "    # train, test = input_data[:-1], input_data[-1:]\n",
    "    result_df = pd.DataFrame(input_data)\n",
    "    result_df[\"label\"] = 1 # tmp for now\n",
    "    result_df[\"hist_len\"] = SEQ_LEN\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36997594-08a0-4e25-9d9a-4d41508fb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/ml-25m/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f095318a-9ccf-445c-995e-0a5cfd92311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"./datasets/ml-25m/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8610d393-91c6-467d-8462-3e6156fe227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd033df-aca9-4646-861d-d5d42a187ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab3f4cf6-b981-458d-ad76-272e5d7daa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(df,movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c65343-9207-4067-9598-ef6cd7e57535",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"genres\"] = data[\"genres\"].apply(lambda x:x.split(\"|\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d092e29-fcce-482e-92b6-97828f3a0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\", \"title\", \"genres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2346888-f2df-45b0-a8f6-37cfbac6e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe = LabelEncoder()\n",
    "data[\"genres\"] = lbe.fit_transform(data[\"genres\"]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8f7d16-7777-417b-bcc0-5c5063528efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d57627a-d7bb-4b21-925a-2ed4bdf64116",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "275528f0-f73f-4ae3-af32-5d1e5f88b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "609654ff-b809-489b-a9e9-166bae43767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35e4510f-2b12-405e-ac4d-9f930460b3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5b7e174-d0dd-47cb-b0ef-17b7873d3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_size =  data['user_id'].max()+1\n",
    "movie_id_size = data['movie_id'].max()+1\n",
    "genre_id_size = int(data['genres'].max())+1\n",
    "embedding_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8050d661-f12a-4576-b13d-248e1c05b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_columns = [\n",
    "                        # SparseFeat('user_id', user_id_size, 16),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_movie_id', movie_id_size, embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_genres', genre_id_size, embedding_dim,\n",
    "                                                   embedding_name=\"genres\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "item_feature_columns = [SparseFeat('movie_id', movie_id_size, embedding_dim),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8538a905-be86-4e59-9601-99df5224ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_df = data[:1000000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96093ebf-4432-4a6b-8a1d-848d3792a79d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 162541/162541 [08:17<00:00, 326.53it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set = data.groupby(\"user_id\").progress_apply(lambda x:get_train_input_for_user(x[:80]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b94bd8d4-b5aa-4f5b-bf30-f58bd31429f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 162541/162541 [00:51<00:00, 3164.51it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = data.groupby(\"user_id\").progress_apply(lambda x:get_test_input_for_user(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce6c5fbf-7bca-400b-87de-fa7560ce27f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set[\"history_movie_id\"] = pad_sequences(test_set[\"history_movie_id\"], maxlen=SEQ_LEN, padding='post', truncating='post', value=0).tolist()\n",
    "test_set[\"history_genre_id\"] = pad_sequences(test_set[\"history_genre_id\"], maxlen=SEQ_LEN, padding='post', truncating='post', value=0).tolist()\n",
    "\n",
    "test_input = test_set.sample(frac=1)\n",
    "\n",
    "test_model_input = {\n",
    "    \"hist_movie_id\":np.array(test_input[\"history_movie_id\"].tolist()),\n",
    "    \"hist_genres\":np.array(test_input[\"history_genre_id\"].tolist()),\n",
    "    \"movie_id\": np.array(test_input[\"movie_id\"].tolist()),\n",
    "    \"hist_len\": np.array(test_input[\"hist_len\"].tolist()),\n",
    "}\n",
    "test_label =  test_input[\"label\"].tolist()\n",
    "\n",
    "train_set[\"history_movie_id\"] = pad_sequences(train_set[\"history_movie_id\"], maxlen=SEQ_LEN, padding='post', truncating='post', value=0).tolist()\n",
    "train_set[\"history_genre_id\"] = pad_sequences(train_set[\"history_genre_id\"], maxlen=SEQ_LEN, padding='post', truncating='post', value=0).tolist()\n",
    "\n",
    "train_input = train_set.sample(frac=1)\n",
    "\n",
    "train_model_input = {\n",
    "    \"hist_movie_id\":np.array(train_input[\"history_movie_id\"].tolist()),\n",
    "    \"hist_genres\":np.array(train_input[\"history_genre_id\"].tolist()),\n",
    "    \"movie_id\": np.array(train_input[\"movie_id\"].tolist()),\n",
    "    \"hist_len\": np.array(train_input[\"hist_len\"].tolist()),\n",
    "}\n",
    "train_label =  train_input[\"label\"].tolist()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae71d535-ae26-44f5-b7a0-2aa3cd74393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__ >= '2.0.0':\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "else:\n",
    "    K.set_learning_phase(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bfe610c-3998-46fc-b48b-4ff7e6e16556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/.cache/pypoetry/virtualenvs/cinepaw-P6zxCTLW-py3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 23:26:33.600176: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-03-29 23:26:33.600191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pop-os\n",
      "2024-03-29 23:26:33.600193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pop-os\n",
      "2024-03-29 23:26:33.600270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.89.2\n",
      "2024-03-29 23:26:33.600277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.89.2\n",
      "2024-03-29 23:26:33.600278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.89.2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "train_counter = Counter(data['movie_id'])\n",
    "item_count = [train_counter.get(i,0) for i in range(item_feature_columns[0].vocabulary_size)] #todo change this later\n",
    "sampler_config = NegativeSampler('frequency',num_sampled=255,item_name=\"movie_id\",item_count=item_count)\n",
    "#model = YoutubeDNN(user_feature_columns, item_feature_columns, user_dnn_hidden_units=(128,64, embedding_dim), sampler_config=sampler_config)\n",
    "model = ComiRec(user_feature_columns,\n",
    "                item_feature_columns,\n",
    "                k_max=2, \n",
    "                user_dnn_hidden_units=(128,64, embedding_dim),\n",
    "                sampler_config=sampler_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cb412-f8a0-4336-bf6d-05aa32fc12f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9617397 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 23:26:37.242782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 23:26:37.269088: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2024-03-29 23:26:37.332414: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/user_dnn/bias0/v/Assign' id:1298 op device:{requested: '', assigned: ''} def:{{{node training/Adam/user_dnn/bias0/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/user_dnn/bias0/v, training/Adam/user_dnn/bias0/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9617397/9617397 [==============================] - 3110s 323us/sample - loss: 3.8392\n",
      "Epoch 13/20\n",
      "9617397/9617397 [==============================] - 3113s 324us/sample - loss: 3.8321\n",
      "Epoch 14/20\n",
      "9617397/9617397 [==============================] - 3113s 324us/sample - loss: 3.8267\n",
      "Epoch 15/20\n",
      "8660480/9617397 [==========================>...] - ETA: 5:09 - loss: 3.8222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9617397/9617397 [==============================] - 3111s 323us/sample - loss: 3.8148\n",
      "Epoch 18/20\n",
      "9617397/9617397 [==============================] - 3116s 324us/sample - loss: 3.8115\n",
      "Epoch 19/20\n",
      "8528896/9617397 [=========================>....] - ETA: 5:53 - loss: 3.8078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 109056/9617397 [..............................] - ETA: 51:35 - loss: 3.7958"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n",
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=512, epochs=20, verbose=1, validation_split=0.0, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b153c-de2d-4f60-b940-5bfbbef8f100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af971f-598d-421e-aeaa-459513860e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18d6a8-a2e1-4e98-9d83-dd2524934b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6aa66-2b55-4e30-b949-b56fc0f8f208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2663b-60da-4644-adc1-b6084ba702e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e57d93-962f-4cde-8f62-fa8ee6ab390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[movies[\"title\"].apply(lambda x:\"The Shining\" in x)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peotry-rec",
   "language": "python",
   "name": "peotry-rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
