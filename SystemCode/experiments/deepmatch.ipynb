{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12a6752-6866-49a2-9714-6849a024cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06e9e19c-9cb0-4330-8217-04fc7a0d8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "def gen_data_set(data, seq_max_len=50, negsample=0):\n",
    "    data.sort_values(\"timestamp\", inplace=True)\n",
    "    item_ids = data['movie_id'].unique()\n",
    "    item_id_genres_map = dict(zip(data['movie_id'].values, data['genres'].values))\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        pos_list = hist['movie_id'].tolist()\n",
    "        genres_list = hist['genres'].tolist()\n",
    "        rating_list = hist['rating'].tolist()\n",
    "\n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set, size=len(pos_list) * negsample, replace=True)\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            genres_hist = genres_list[:i]\n",
    "            seq_len = min(i, seq_max_len)\n",
    "            if i != len(pos_list) - 1:\n",
    "                train_set.append((\n",
    "                    reviewerID, pos_list[i], 1, hist[::-1][:seq_len], seq_len, genres_hist[::-1][:seq_len],\n",
    "                    genres_list[i],\n",
    "                    rating_list[i]))\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, neg_list[i * negsample + negi], 0, hist[::-1][:seq_len], seq_len,\n",
    "                                      genres_hist[::-1][:seq_len], item_id_genres_map[neg_list[i * negsample + negi]]))\n",
    "            else:\n",
    "                test_set.append((reviewerID, pos_list[i], 1, hist[::-1][:seq_len], seq_len, genres_hist[::-1][:seq_len],\n",
    "                                 genres_list[i],\n",
    "                                 rating_list[i]))\n",
    "\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    print(len(train_set[0]), len(test_set[0]))\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "def gen_model_input(train_set, user_profile, seq_max_len):\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_iid = np.array([line[1] for line in train_set])\n",
    "    train_label = np.array([line[2] for line in train_set])\n",
    "    train_seq = [line[3] for line in train_set]\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "    train_seq_genres = [line[5] for line in train_set]\n",
    "    train_genres = np.array([line[6] for line in train_set])\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_seq_genres_pad = pad_sequences(train_seq_genres, maxlen=seq_max_len, padding='post', truncating='post',\n",
    "                                         value=0)\n",
    "    train_model_input = {\"user_id\": train_uid, \"movie_id\": train_iid, \"hist_movie_id\": train_seq_pad,\n",
    "                         \"hist_genres\": train_seq_genres_pad,\n",
    "                         \"hist_len\": train_hist_len, \"genres\": train_genres}\n",
    "\n",
    "    for key in [\"gender\", \"age\", \"occupation\", \"zip\"]:\n",
    "        train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
    "\n",
    "    return train_model_input, train_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58c6a660-66f4-4624-87ad-590b5ba8f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepmatch.models import ComiRec\n",
    "from deepmatch.utils import sampledsoftmaxloss, NegativeSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70ba27b8-3550-4913-b64e-20f1df351abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = tf.keras.utils.pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "628b0a0b-c531-469a-9233-b767dfe867a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2623709/3090652750.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  user = pd.read_csv(data_path+'ml-1m/users.dat',sep='::',header=None,names=unames)\n",
      "/tmp/ipykernel_2623709/3090652750.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings = pd.read_csv(data_path+'ml-1m/ratings.dat',sep='::',header=None,names=rnames)\n",
      "/tmp/ipykernel_2623709/3090652750.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  movies = pd.read_csv(data_path+'ml-1m/movies.dat',sep='::',header=None,names=mnames,encoding=\"unicode_escape\")\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./datasets/\"\n",
    "\n",
    "unames = ['user_id','gender','age','occupation','zip']\n",
    "user = pd.read_csv(data_path+'ml-1m/users.dat',sep='::',header=None,names=unames)\n",
    "rnames = ['user_id','movie_id','rating','timestamp']\n",
    "ratings = pd.read_csv(data_path+'ml-1m/ratings.dat',sep='::',header=None,names=rnames)\n",
    "mnames = ['movie_id','title','genres']\n",
    "movies = pd.read_csv(data_path+'ml-1m/movies.dat',sep='::',header=None,names=mnames,encoding=\"unicode_escape\")\n",
    "movies['genres'] = list(map(lambda x: x.split('|')[0], movies['genres'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "25962ca1-80b7-482b-beab-772d6d869670",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(pd.merge(ratings,movies),user)#.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fc024fb5-7a45-432a-b003-ae3e31869bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [\"movie_id\", \"user_id\",\n",
    "                    \"gender\", \"age\", \"occupation\", \"zip\", \"genres\"]\n",
    "SEQ_LEN = 50\n",
    "negsample = 0\n",
    "\n",
    "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
    "feature_max_idx = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "afc1c91b-c906-4248-a0c1-b68defb764ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 3]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2], [3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6bdd7eaf-0a3d-4a7b-a317-7d21a6eaf046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 6040/6040 [00:03<00:00, 1514.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "\n",
    "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id')\n",
    "\n",
    "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
    "\n",
    "user_profile.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)\n",
    "\n",
    "train_set, test_set = gen_data_set(data, SEQ_LEN, negsample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eb8431bb-9cec-44a1-97b1-a7e95f09bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
    "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
    "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
    "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
    "                        SparseFeat(\"zip\", feature_max_idx['zip'], 16),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_genres', feature_max_idx['genres'], embedding_dim,\n",
    "                                                   embedding_name=\"genres\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "164ab187-7773-45a4-9071-6a3a6204c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "50e5aed7-2bc2-48b5-bacd-9bd43cdd9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "train_counter = Counter(train_model_input['movie_id'])\n",
    "#train_counter = Counter(data[\"movie_id\"])\n",
    "item_count = [train_counter.get(i,0) for i in range(item_feature_columns[0].vocabulary_size)]\n",
    "sampler_config = NegativeSampler('frequency',num_sampled=255,item_name=\"movie_id\",item_count=item_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a08f62c2-d0cb-44ea-be02-d722f6e262fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__ >= '2.0.0':\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "else:\n",
    "    K.set_learning_phase(True)\n",
    "    \n",
    "#model = YoutubeDNN(user_feature_columns, item_feature_columns, user_dnn_hidden_units=(128,64, embedding_dim), sampler_config=sampler_config)\n",
    "model = ComiRec(user_feature_columns,item_feature_columns,k_max=2, user_dnn_hidden_units=(128,64, embedding_dim), sampler_config=sampler_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6d27d118-77e9-49ad-b714-3a4686d55395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4bd8f199-2d54-4601-8a26-7417303ee4ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 988129 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 23:40:22.976522: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/user_dnn_3/bias2/v/Assign' id:5606 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/user_dnn_3/bias2/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/user_dnn_3/bias2/v, training_2/Adam/user_dnn_3/bias2/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988129/988129 [==============================] - 22s 23us/sample - loss: 5.1416\n",
      "Epoch 2/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 4.6740\n",
      "Epoch 3/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 4.4308\n",
      "Epoch 4/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 4.1622\n",
      "Epoch 5/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 4.0298\n",
      "Epoch 6/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.9395\n",
      "Epoch 7/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.8690\n",
      "Epoch 8/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.8105\n",
      "Epoch 9/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.7648\n",
      "Epoch 10/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.7290\n",
      "Epoch 11/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.6978\n",
      "Epoch 12/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.6709\n",
      "Epoch 13/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.6476\n",
      "Epoch 14/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.6278\n",
      "Epoch 15/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.6103\n",
      "Epoch 16/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.5941\n",
      "Epoch 17/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.5802\n",
      "Epoch 18/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.5674\n",
      "Epoch 19/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.5550\n",
      "Epoch 20/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.5452\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=512, epochs=20, verbose=1, validation_split=0.0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b2a6f280-b122-4078-b0e4-a63a5a85502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 2, 32)\n",
      "(3706, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/cinepaw-P6zxCTLW-py3.10/lib/python3.10/site-packages/tensorflow/python/keras/engine/training.py:2455: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2024-03-24 23:52:19.366761: W tensorflow/c/c_api.cc:291] Operation '{name:'user_dnn_3/dropout_19/cond/Identity' id:4380 op device:{requested: '', assigned: ''} def:{{{node user_dnn_3/dropout_19/cond/Identity}} = Identity[T=DT_FLOAT, _has_manual_control_dependencies=true](user_dnn_3/dropout_19/cond)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-03-24 23:52:19.503958: W tensorflow/c/c_api.cc:291] Operation '{name:'lambda_11/Squeeze' id:4562 op device:{requested: '', assigned: ''} def:{{{node lambda_11/Squeeze}} = Squeeze[T=DT_FLOAT, _has_manual_control_dependencies=true, squeeze_dims=[1]](lambda_11/GatherV2)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "test_user_model_input = test_model_input\n",
    "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values,}\n",
    "\n",
    "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8da89-a8a1-4f47-a4cf-3992e288e255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "090b5487-a364-4460-8ce3-92fe503ee813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import  save_model,load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "83d8dbd0-3516-498d-9371-cb0056aa6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepmatch.layers import custom_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "83dcb84d-e89d-4ea6-83d6-732810f691e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(user_embedding_model, 'user_emb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "72de4c57-2549-4b55-9ec7-d297442adc14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 23:54:17.997428: W tensorflow/c/c_api.cc:291] Operation '{name:'sparse_seq_emb_hist_genres_4/embeddings/Assign' id:5784 op device:{requested: '', assigned: ''} def:{{{node sparse_seq_emb_hist_genres_4/embeddings/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](sparse_seq_emb_hist_genres_4/embeddings, sparse_seq_emb_hist_genres_4/embeddings/Initializer/stateless_random_normal)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "user_embedding_model = load_model('user_emb.h5',custom_objects)# load_model,just add a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "42ceb590-21a3-404b-aa2f-30aa5a2de6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 2, 32)\n",
      "(3706, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/cinepaw-P6zxCTLW-py3.10/lib/python3.10/site-packages/tensorflow/python/keras/engine/training.py:2455: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2024-03-24 23:54:18.724928: W tensorflow/c/c_api.cc:291] Operation '{name:'user_dnn_4/dropout_24/cond/Identity' id:6166 op device:{requested: '', assigned: ''} def:{{{node user_dnn_4/dropout_24/cond/Identity}} = Identity[T=DT_FLOAT, _has_manual_control_dependencies=true](user_dnn_4/dropout_24/cond)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "# user_embs = user_embs[:, i, :]  # i in [0,k_max) if MIND\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "91d01509-aedf-4f87-a6db-3a9edafcf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "from deepmatch.utils import recall_N\n",
    "\n",
    "k_max = 2\n",
    "topN = 50\n",
    "test_true_label = {line[0]: [line[1]] for line in test_set}\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "# faiss.normalize_L2(item_embs)\n",
    "index.add(item_embs)\n",
    "# faiss.normalize_L2(user_embs)\n",
    "\n",
    "if len(user_embs.shape) == 2:  # multi interests model's shape = 3 (MIND,ComiRec)\n",
    "    user_embs = np.expand_dims(user_embs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "47eb75ba-b67c-4d84-bedc-bfa22800ae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 6040/6040 [00:00<00:00, 21827.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 6040/6040 [00:00<00:00, 17698.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.42549668874172186\n",
      "hr 0.42549668874172186\n"
     ]
    }
   ],
   "source": [
    "score_dict = defaultdict(dict)\n",
    "for k in range(k_max):\n",
    "    user_emb = user_embs[:, k, :]\n",
    "    D, I = index.search(np.ascontiguousarray(user_emb), topN)\n",
    "    for i, uid in tqdm(enumerate(test_user_model_input['user_id']), total=len(test_user_model_input['user_id'])):\n",
    "        if np.abs(user_emb[i]).max() < 1e-8:\n",
    "            continue\n",
    "        for score, itemid in zip(D[i], I[i]):\n",
    "            score_dict[uid][itemid] = max(score, score_dict[uid].get(itemid, float(\"-inf\")))\n",
    "\n",
    "s = []\n",
    "hit = 0\n",
    "for i, uid in enumerate(test_user_model_input['user_id']):\n",
    "    pred = [item_profile['movie_id'].values[x[0]] for x in\n",
    "            heapq.nlargest(topN, score_dict[uid].items(), key=lambda x: x[1])]\n",
    "    filter_item = None\n",
    "    recall_score = recall_N(test_true_label[uid], pred, N=topN)\n",
    "    s.append(recall_score)\n",
    "    if test_true_label[uid] in pred:\n",
    "        hit += 1\n",
    "\n",
    "print(\"recall\", np.mean(s))\n",
    "print(\"hr\", hit / len(test_user_model_input['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ae304-5298-4aec-99e7-3d660e37e394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f76ad-425a-4653-87fb-599cba7bdf71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a4b0000e-8463-4d02-aca0-2e2574dcd3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2159, 32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9fd2347a-f255-48cf-8797-5edcf3c830b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_user_model_input[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "05655bb5-d76d-4f34-b7b2-af4d4101938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_model_input['hist_movie_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0c9265a2-4ea5-4497-8d55-64ac7f7a6e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2159, 32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b5d02-07a4-4cd9-843f-f7ba1592c5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f23fbb-dd5c-40bf-b708-da45db654d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e0823e7-093f-4a52-a8a4-ff430269c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uid = np.array([line[0] for line in train_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd1ee4c9-4a8b-4c47-b5bd-63178929773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [line[5] for line in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d5001c9-b174-46db-9635-0e601935db11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set[4][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "540b1bea-d046-4102-9763-f555e4eb98c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.array([line[5] for line in train_set[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f284ac78-0e0b-4655-8918-6e8b9b87c86e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9860,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_seq_genres \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9860,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "train_seq_genres = np.array([line[5] for line in train_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c59e7dcb-f973-4abb-a4c9-7820f2442ee3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9860,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model_input, train_label \u001b[38;5;241m=\u001b[39m \u001b[43mgen_model_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_model_input, test_label \u001b[38;5;241m=\u001b[39m gen_model_input(test_set, user_profile, SEQ_LEN)\n",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36mgen_model_input\u001b[0;34m(train_set, user_profile, seq_max_len)\u001b[0m\n\u001b[1;32m     46\u001b[0m train_seq \u001b[38;5;241m=\u001b[39m [line[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m train_set]\n\u001b[1;32m     47\u001b[0m train_hist_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([line[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m train_set])\n\u001b[0;32m---> 48\u001b[0m train_seq_genres \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m train_genres \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([line[\u001b[38;5;241m6\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m train_set])\n\u001b[1;32m     50\u001b[0m train_seq_pad \u001b[38;5;241m=\u001b[39m pad_sequences(train_seq, maxlen\u001b[38;5;241m=\u001b[39mseq_max_len, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, truncating\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9860,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c73b59d-09b9-43b6-96cf-b497b61a872f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m item_feature_columns \u001b[38;5;241m=\u001b[39m [SparseFeat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_max_idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m], embedding_dim)]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m---> 19\u001b[0m train_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[43mtrain_model_input\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     20\u001b[0m item_count \u001b[38;5;241m=\u001b[39m [train_counter\u001b[38;5;241m.\u001b[39mget(i,\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(item_feature_columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvocabulary_size)]\n\u001b[1;32m     21\u001b[0m sampler_config \u001b[38;5;241m=\u001b[39m NegativeSampler(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m,num_sampled\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m,item_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,item_count\u001b[38;5;241m=\u001b[39mitem_count)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model_input' is not defined"
     ]
    }
   ],
   "source": [
    "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
    "\n",
    "embedding_dim = 32\n",
    "\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
    "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
    "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
    "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
    "                        SparseFeat(\"zip\", feature_max_idx['zip'], 16),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_genres', feature_max_idx['genres'], embedding_dim,\n",
    "                                                   embedding_name=\"genres\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]\n",
    "\n",
    "from collections import Counter\n",
    "train_counter = Counter(train_model_input['movie_id'])\n",
    "item_count = [train_counter.get(i,0) for i in range(item_feature_columns[0].vocabulary_size)]\n",
    "sampler_config = NegativeSampler('frequency',num_sampled=255,item_name=\"movie_id\",item_count=item_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75362e27-cff6-498e-9639-e94d4c6f57f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peotry-rec",
   "language": "python",
   "name": "peotry-rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
